{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of possible Chinese characters that are pronounced as yi is 484\n"
     ]
    }
   ],
   "source": [
    "## Read in the Chinese character pinyin map in a dictionary\n",
    "Chinese_character_pinyin_map = {}\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/chinese/charmap',encoding='utf8') as Chinese_charmap:\n",
    "    for line in Chinese_charmap:\n",
    "        Chinese_character_pinyin_map[line[0]] = line[1:].strip()\n",
    "        \n",
    "count_yi = 0\n",
    "for char in Chinese_character_pinyin_map:\n",
    "    if Chinese_character_pinyin_map[char] == 'yi':\n",
    "        count_yi+=1\n",
    "        \n",
    "print ('The number of possible Chinese characters that are pronounced as yi is {}'.format(count_yi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n",
      "['啊', '阿', '嗄', '锕', 'a']\n"
     ]
    }
   ],
   "source": [
    "## Construct a dict, and the key of the dict is the pinyin in Charmap, and the value is a list contains all the Chinese\n",
    "## characters matching that pinyin\n",
    "Chinese_pinyin_character_map = {}\n",
    "for char in Chinese_character_pinyin_map:\n",
    "    if Chinese_character_pinyin_map[char] not in Chinese_pinyin_character_map:\n",
    "        Chinese_pinyin_character_map[Chinese_character_pinyin_map[char]] = [char]\n",
    "    else:\n",
    "        Chinese_pinyin_character_map[Chinese_character_pinyin_map[char]].append(char)\n",
    "\n",
    "for pron in Chinese_pinyin_character_map:\n",
    "    if len(pron) == 1:\n",
    "        Chinese_pinyin_character_map[pron].append(pron)\n",
    "        \n",
    "print (len(Chinese_pinyin_character_map['yi']))\n",
    "print (Chinese_pinyin_character_map['a'])\n",
    "#print (Chinese_pinyin_character_map['yi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read in the Chinese training file as a big string\n",
    "Chinese_train_string = ''\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/chinese/train.han',encoding='utf8') as Chinese_train:\n",
    "    for line in Chinese_train:\n",
    "        Chinese_train_string += line\n",
    "#print (Chinese_train_string[0:100])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Construct unigram word count for the train string\n",
    "word_count_unigram = {} ## This contains the word counts for unigram in Chinese_train file\n",
    "for cha in Chinese_train_string:\n",
    "    if cha not in word_count_unigram:\n",
    "        word_count_unigram[cha]=1\n",
    "    else:\n",
    "        word_count_unigram[cha]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define function to create the tree structure to store the training parameters\n",
    "def create(ngram, dic):## ngram is a ngram string, dic is a dictionary. \n",
    "    n = len(ngram)\n",
    "    if n == 0: ##base case \n",
    "        return\n",
    "    if ngram[0] not in dic:\n",
    "        dic[ngram[0]] = []\n",
    "        dic[ngram[0]].append(1)\n",
    "        temp_dict = {}\n",
    "        dic[ngram[0]].append(temp_dict)\n",
    "    else:\n",
    "        dic[ngram[0]][0] += 1\n",
    "    create(ngram[1:],dic[ngram[0]][1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train the n-gram model\n",
    "n = 9  ## 10-gram model\n",
    "ngram_training = {} ## This is tree structure containing the traing results\n",
    "                    ## Key is the cha, then value is a list; 1st element in the list is the count of the key,\n",
    "                    ## 2nd element in the list is a dict in which key is the cha left following the previous key\n",
    "                    ## for n gram, there are n levels of the tree\n",
    "\n",
    "for i in range(len(Chinese_train_string)-(n-1)):\n",
    "    ngram = ''\n",
    "    for j in range(n):\n",
    "        ngram +=  Chinese_train_string[i+j]\n",
    "    create(ngram, ngram_training)\n",
    "\n",
    "#Deal with the last a few characters \n",
    "n = n-1\n",
    "while n!=0 :\n",
    "    create(Chinese_train_string[-n:],ngram_training)\n",
    "    n-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to find the counts in the tree structure. This function traverse the tree to find the counts of ngram  \n",
    "def find_counts (ngram, ngram_training): ## ngram_training is a tree structure for n = 10, so len(ngram) should be less than 10\n",
    "    n = len(ngram)                       ## find the counts of ngram\n",
    "    if n == 1:  ## base case\n",
    "        if ngram not in ngram_training:\n",
    "            return 0\n",
    "        else:\n",
    "            return ngram_training[ngram][0]    \n",
    "    else:\n",
    "        if ngram[0] not in ngram_training:\n",
    "            return 0\n",
    "        else:\n",
    "            return find_counts(ngram[1:],ngram_training[ngram[0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to find the subtree \n",
    "def find_subtree (ngram, ngram_training): ## ngram_training is a tree structure for n = 10, so len(ngram) should be less than 10\n",
    "    n = len(ngram)\n",
    "    if n == 1:\n",
    "        if ngram not in ngram_training:\n",
    "            temp ={}\n",
    "            return temp\n",
    "        else:\n",
    "            return ngram_training[ngram][1]    \n",
    "    else:\n",
    "        if ngram[0] not in ngram_training:\n",
    "            temp = {}\n",
    "            return temp\n",
    "        else:\n",
    "            return find_subtree(ngram[1:],ngram_training[ngram[0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define the recursive function for prediction with witten-bell smoothing\n",
    "## This function returns p(w/u)\n",
    "def prediction_witten_bell_smoothing(n,w,position_predict,word_string_han): ## n means n-gram, w is the character to predict, \n",
    "                                                                        ## position_predict is the postion to be predicted                                                                       \n",
    "    ## construct u according to the correct han characters     \n",
    "    u=''\n",
    "    for i in range(n-1,0,-1):\n",
    "        u += word_string_han[position_predict-i]   \n",
    "    \n",
    "        \n",
    "    if len(u)==0: ## base case, return p(w)  \n",
    "        N = len(Chinese_train_string) # number of word token-not including <s>\n",
    "        n_1_plus = len(word_count_unigram)## number of word type seen at least one time -not including <s>\n",
    "        n_word_type = len(word_count_unigram)+1 ## number of word type_include unkown word\n",
    "        lamda = N/(N+n_1_plus)\n",
    "        if w in word_count_unigram:\n",
    "            return lamda*ngram_training[w][0]/N + (1-lamda)/n_word_type   ## ngram_training[w][0] represents c(w), return p(w)\n",
    "        else:\n",
    "            return (1-lamda)/n_word_type\n",
    "        \n",
    "    if find_counts (u, ngram_training) == 0:\n",
    "        return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string_han)\n",
    "    else:        \n",
    "        ## calculate c(u_dot) and n1+(u_dot)        \n",
    "        sub_tree_under_u = find_subtree (u, ngram_training)\n",
    "        count_u_dot = 0\n",
    "        for i in sub_tree_under_u:\n",
    "            count_u_dot+=sub_tree_under_u[i][0]     \n",
    "        \n",
    "        count_1plus_u_dot = 0\n",
    "        for i in sub_tree_under_u:\n",
    "            count_1plus_u_dot+=1 \n",
    "            \n",
    "        if count_u_dot == 0:\n",
    "            return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string_han)\n",
    "                \n",
    "        ## calculate lamda_u\n",
    "        lamda_u = count_u_dot/(count_u_dot+count_1plus_u_dot)\n",
    "        \n",
    "        ## calculate c(uw)\n",
    "        c_uw = find_counts(u+w, ngram_training)\n",
    "        \n",
    "        return lamda_u*c_uw/count_u_dot+(1-lamda_u)*prediction_witten_bell_smoothing(n-1,w,position_predict,word_string_han)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read in dev.pin as a big string, and convert to list with all the words \n",
    "Chinese_dev_pinyin_string = \"\"\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/chinese/dev.pin',encoding='utf8') as Chinese_dev_pinyin:\n",
    "    for line in Chinese_dev_pinyin:\n",
    "        Chinese_dev_pinyin_string += line\n",
    "        \n",
    "#print (Chinese_dev_pinyin_string[0:100])\n",
    "word_dev_pin_list=re.findall(r'\\S+|\\n',Chinese_dev_pinyin_string)\n",
    "#'\\S+|\\n' will match all the combinations of none whitespace character with length 1 or more (\\S+) or new line (\\n).\n",
    "#word_dev_pin_list = Chinese_dev_pinyin_string.split()\n",
    "#print (word_dev_pin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816\n"
     ]
    }
   ],
   "source": [
    "## Read in dev.han as a big string, remove all the newline character \n",
    "Chinese_dev_han_string = \"\"\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/chinese/dev.han',encoding='utf8') as Chinese_dev_han:\n",
    "    for line in Chinese_dev_han:\n",
    "        Chinese_dev_han_string += line\n",
    "print (len(Chinese_dev_han_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9242256637168141\n"
     ]
    }
   ],
   "source": [
    "## Make prediction on the dev file\n",
    "n=9\n",
    "Chinese_prediction = {}\n",
    "for position in range(n-1, len(word_dev_pin_list)):\n",
    "    prediction_word = {}\n",
    "    if word_dev_pin_list[position] == '<space>':\n",
    "        Chinese_prediction[position] = \" \"\n",
    "    else:\n",
    "        if word_dev_pin_list[position] in Chinese_pinyin_character_map:\n",
    "            for word in Chinese_pinyin_character_map[word_dev_pin_list[position]]: \n",
    "                prediction_word[word] = prediction_witten_bell_smoothing(n,word,position,Chinese_dev_han_string)\n",
    "            Chinese_prediction[position]=[key for key, val in prediction_word.items() if val == max(prediction_word.values())][0]\n",
    "        else: \n",
    "            Chinese_prediction[position] = word_dev_pin_list[position]\n",
    "            \n",
    "Correct_prediction_count = 0\n",
    "Correct_position = []\n",
    "for position in range(n-1, len(word_dev_pin_list)):\n",
    "    if Chinese_prediction[position] == Chinese_dev_han_string[position]:\n",
    "        Correct_prediction_count+=1\n",
    "        Correct_position.append(position)\n",
    "\n",
    "print (Correct_prediction_count/(len(Chinese_dev_han_string)-(n-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646\n"
     ]
    }
   ],
   "source": [
    "## Read in test.pin as a big string, and convert to list with all the words \n",
    "Chinese_test_pinyin_string = \"\"\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/chinese/test.pin',encoding='utf8') as Chinese_test_pinyin:\n",
    "    for line in Chinese_test_pinyin:\n",
    "        Chinese_test_pinyin_string += line\n",
    "        \n",
    "word_test_pin_list = re.findall(r'\\S+|\\n',Chinese_test_pinyin_string)\n",
    "print (len(word_test_pin_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646\n"
     ]
    }
   ],
   "source": [
    "## Read in dev.han as a big string, remove all the newline character \n",
    "Chinese_test_han_string = \"\"\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/chinese/test.han',encoding='utf8') as Chinese_test_han:\n",
    "    for line in Chinese_test_han:\n",
    "        Chinese_test_han_string += line\n",
    "        \n",
    "print (len(Chinese_test_han_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8821733821733821\n"
     ]
    }
   ],
   "source": [
    "## Make prediction on the test file\n",
    "n=9\n",
    "#English_letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
    "                   #'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "\n",
    "Chinese_prediction = {}\n",
    "for position in range(n-1, len(word_test_pin_list)):\n",
    "    prediction_word = {}\n",
    "    if word_test_pin_list[position] == '<space>':\n",
    "        Chinese_prediction[position] = \" \"\n",
    "        \n",
    "    #elif word_test_pin_list[position] in English_letters:\n",
    "        #Chinese_prediction[position] = word_test_pin_list[position]\n",
    "                      \n",
    "    else:        \n",
    "        \n",
    "        if word_test_pin_list[position] in Chinese_pinyin_character_map:\n",
    "            for word in Chinese_pinyin_character_map[word_test_pin_list[position]]: \n",
    "                prediction_word[word] = prediction_witten_bell_smoothing(n,word,position,Chinese_test_han_string)\n",
    "            Chinese_prediction[position]=[key for key, val in prediction_word.items() if val == max(prediction_word.values())][0]\n",
    "        else:\n",
    "            Chinese_prediction[position]=word_test_pin_list[position]\n",
    "            \n",
    "Correct_prediction_count = 0\n",
    "for position in range(n-1, len(word_test_pin_list)):\n",
    "    if Chinese_prediction[position] == Chinese_test_han_string[position]:\n",
    "        Correct_prediction_count+=1\n",
    "\n",
    "print (Correct_prediction_count/(len(Chinese_test_han_string)-(n-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
