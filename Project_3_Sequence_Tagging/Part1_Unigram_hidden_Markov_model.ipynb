{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the 0th hidden Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct the tree structure to count the c(t,w). \n",
    "# Use the dictionary to store the counts\n",
    "# For the dictionary word_tag_pair_counts, the keys are the words in the training set, the value is a dictionary, in which the keys \n",
    "# are the tag, the the value are the counts for this word-key pair \n",
    "word_tag_pair_counts = {}\n",
    "with open (\"C:/Users/Xin/Desktop/Github-repo/Natural_Language_Processing_Projects/Project_3_Sequence_Tagging/hw3-data/train.txt\") as train_file:\n",
    "    for line in train_file:\n",
    "        line_list = line.split()\n",
    "        for i in range(len(line_list)):\n",
    "            temp = line_list[i].split('/')\n",
    "            if temp[0] not in word_tag_pair_counts:\n",
    "                word_tag_pair_counts[temp[0]] = {}\n",
    "                word_tag_pair_counts[temp[0]][temp[1]] = 1\n",
    "            else:\n",
    "                if temp[1] not in word_tag_pair_counts[temp[0]]:\n",
    "                    word_tag_pair_counts[temp[0]][temp[1]] = 1\n",
    "                else:\n",
    "                    word_tag_pair_counts[temp[0]][temp[1]] += 1 \n",
    "                    \n",
    "\n",
    "#word_tag_pair_counts\n",
    "\n",
    "#{'spoke': {'N': 5},\n",
    "# 'finished': {'A': 1, 'N': 49, 'R': 2},\n",
    "# 'sucker': {'N': 4},\n",
    "# 'sideline': {'R': 1},\n",
    "#  ...\n",
    "#  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Construct bag of word for word and tag seperately\n",
    "word_count = {}\n",
    "tag_count = {}\n",
    "with open (\"C:/Users/Xin/Desktop/Github-repo/Natural_Language_Processing_Projects/Project_3_Sequence_Tagging/hw3-data/train.txt\") as train_file:\n",
    "    for line in train_file:\n",
    "        line_list = line.split()\n",
    "        for i in range(len(line_list)):\n",
    "            temp = line_list[i].split('/')\n",
    "            if temp[0] not in word_count:\n",
    "                word_count[temp[0]]=1\n",
    "            if temp[0] in word_count:\n",
    "                word_count[temp[0]]+=1\n",
    "            if temp[1] not in tag_count:\n",
    "                tag_count[temp[1]]=1\n",
    "            if temp[1] in tag_count:\n",
    "                tag_count[temp[1]]+=1\n",
    "\n",
    "#word_count = {}\n",
    "#{'spoke': 6,\n",
    "# 'finished': 53,\n",
    "# 'sucker': 5,\n",
    "# ...\n",
    "# ...\n",
    "\n",
    "#tag_count = {}\n",
    "# {'A': 3031, 'D': 65382, 'E': 8625, 'F': 78056, 'N': 1333610, 'R': 101735}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make predcition on test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in the test set \n",
    "## Store the all the word in the big list sequently\n",
    "## Store their corresponding tag in another big list\n",
    "with open (\"C:/Users/Xin/Desktop/Github-repo/Natural_Language_Processing_Projects/Project_3_Sequence_Tagging/hw3-data/test.txt\") as test_file:\n",
    "    word_test = []  \n",
    "    tag_test = [] \n",
    "    for line in test_file:\n",
    "        line_list = line.split() ## by default, using whitespace as delimiter\n",
    "        for i in range(len(line_list)):\n",
    "            temp = line_list[i].split('/')\n",
    "            word_test.append(temp[0])\n",
    "            tag_test.append(temp[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make prediction\n",
    "predicted_tag_test = [] ## Store the predicted tags\n",
    "for word in word_test:\n",
    "    if word not in word_count:\n",
    "        predicted_tag_test.append('N')\n",
    "    else:\n",
    "        temp_countainer = {}\n",
    "        sum_tag_word = 0\n",
    "        for tag in word_tag_pair_counts[word]:\n",
    "            sum_tag_word += word_tag_pair_counts[word][tag]\n",
    "        for tag in word_tag_pair_counts[word]:            \n",
    "            freq = word_tag_pair_counts[word][tag]/sum_tag_word\n",
    "            temp_countainer[tag] = freq\n",
    "        ## Obtain the argmax(t) \n",
    "        predicted_t = [key for key, val in temp_countainer.items() if val == max (temp_countainer.values())]\n",
    "        predicted_tag_test.append(predicted_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurary on the test dataset is 0.8597466324013867\n"
     ]
    }
   ],
   "source": [
    "## Calculate the prediction accuracy\n",
    "correct_counts = 0\n",
    "for i in range(len(tag_test)):\n",
    "    if predicted_tag_test[i] == tag_test[i]:\n",
    "        correct_counts+=1\n",
    "print (\"The accurary on the test dataset is {}\".format(correct_counts/len(tag_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain the probability of word \"You\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.0008481764206955047,\n",
       " 'D': 0.4355385920271416,\n",
       " 'E': 0.00024233612019871562,\n",
       " 'F': 3.0292015024839453e-05,\n",
       " 'N': 0.5234460196292258,\n",
       " 'R': 0.03989458378771356}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_you = {}\n",
    "sum_tag_you = 0\n",
    "for tag in word_tag_pair_counts[\"you\"]:\n",
    "    sum_tag_you += word_tag_pair_counts['you'][tag]\n",
    "for tag in word_tag_pair_counts[\"you\"]:\n",
    "    freq = word_tag_pair_counts[\"you\"][tag]/sum_tag_you\n",
    "    probability_you[tag]=freq\n",
    "probability_you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the probability of each tag for word 'you' is: 1.0\n"
     ]
    }
   ],
   "source": [
    "sum_tag_you= 0\n",
    "for tag in probability_you:\n",
    "    sum_tag_you += probability_you[tag]\n",
    "print(\"Sum of the probability of each tag for word 'you' is: {}\".format(sum_tag_you))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Study the second line in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second line of test file is:  huh/F ,/F well/D ,/D um/F ,/F you/D know/D ,/D i/N guess/N it's/N pretty/N deep/N feelings/N ,/N uh/F ,/F\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open (\"C:/Users/Xin/Desktop/Github-repo/Natural_Language_Processing_Projects/Project_3_Sequence_Tagging/hw3-data/test.txt\") as test_file:\n",
    "    first_line = next(test_file)\n",
    "    second_line = next(test_file)\n",
    "    print (\"The second line of test file is: {}\".format(second_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_2ndline = []  \n",
    "line_list = second_line.split() ## by default, using whitespace as delimiter\n",
    "for i in range(len(line_list)):\n",
    "    temp = line_list[i].split('/')\n",
    "    word_2ndline.append(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second line: huh , well , um , you know , i guess it's pretty deep feelings , uh ,\n"
     ]
    }
   ],
   "source": [
    "second_line_notag = \" \".join(word_2ndline)\n",
    "print (\"The second line: {}\".format(second_line_notag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make prediction on second line of test file\n",
    "predicted_tag_test_2ndline = [] ## Store the predicted tags\n",
    "for word in word_2ndline:\n",
    "    if word not in word_count:\n",
    "        predicted_tag_test_2ndline.append('N')\n",
    "    else:\n",
    "        temp_countainer = {}\n",
    "        sum_tag_word = 0\n",
    "        for tag in word_tag_pair_counts[word]:\n",
    "            sum_tag_word += word_tag_pair_counts[word][tag]\n",
    "        for tag in word_tag_pair_counts[word]:            \n",
    "            freq = word_tag_pair_counts[word][tag]/sum_tag_word\n",
    "            temp_countainer[tag] = freq\n",
    "        ## Obtain the argmax(t) \n",
    "        predicted_t = [key for key, val in temp_countainer.items() if val == max (temp_countainer.values())]\n",
    "        predicted_tag_test_2ndline.append(predicted_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second line: huh , well , um , you know , i guess it's pretty deep feelings , uh ,\n",
      "The output tag: ['F', 'N', 'D', 'N', 'F', 'N', 'N', 'D', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'F', 'N']\n"
     ]
    }
   ],
   "source": [
    "print (\"The second line: {}\".format(second_line_notag))\n",
    "print (\"The output tag: {}\".format(predicted_tag_test_2ndline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second line after removing word not labed as 'N':, , , you , i guess it's pretty deep feelings , ,\n"
     ]
    }
   ],
   "source": [
    "## Remove the words not label as 'N'\n",
    "second_line_N = []\n",
    "for i in range(len(word_2ndline)):\n",
    "    if predicted_tag_test_2ndline[i] == 'N':\n",
    "        second_line_N.append(word_2ndline[i])\n",
    "print( \"The second line after removing word not labed as 'N':{}\".format(\" \".join(second_line_N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
