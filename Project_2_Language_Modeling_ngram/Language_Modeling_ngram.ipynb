{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read in the English training file as a big string\n",
    "train_english_string =''\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/english/train') as train_english:\n",
    "    for line in train_english:\n",
    "        train_english_string += line    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct the bag of word for the train string\n",
    "word_count_unigram = {} ## This contains the word counts for unigram\n",
    "for cha in train_english_string:\n",
    "    if cha not in word_count_unigram:\n",
    "        word_count_unigram[cha]=1\n",
    "    else:\n",
    "        word_count_unigram[cha]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct the word string with start word <s>. Using “我” to represent <s>\n",
    "n = 10 \n",
    "start_word=(n-1)*\"我\"  ## 我 represent start word <s>\n",
    "#print (start_word)\n",
    "train_english_string_full = start_word+train_english_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define function to create the tree structure \n",
    "\n",
    "def create(ngram, dic):## ngram is a ngram string, dic is a dictionary. \n",
    "    n = len(ngram)\n",
    "    if n == 0: ##base case \n",
    "        return\n",
    "    if ngram[0] not in dic:\n",
    "        dic[ngram[0]] = []\n",
    "        dic[ngram[0]].append(1)\n",
    "        temp_dict = {}\n",
    "        dic[ngram[0]].append(temp_dict)\n",
    "    else:\n",
    "        dic[ngram[0]][0] += 1\n",
    "    create(ngram[1:],dic[ngram[0]][1]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Train the n-gram model\n",
    "\n",
    "n = 10\n",
    "\n",
    "ngram_training = {} ## This is tree structure containing the traing results\n",
    "                    ## Key is the cha, then value is a list; 1st element in the list is the count of the key,\n",
    "                    ## 2nd element in the list is a dict in which key is the cha left following the previous key\n",
    "                    ## for n gram, there are n levels of the tree\n",
    "#count = 0\n",
    "for i in range(len(train_english_string_full)-(n-1)):\n",
    "    #count +=1\n",
    "    #if count == 1000000:\n",
    "        #print(count)\n",
    "        #count = 0\n",
    "    ngram = ''\n",
    "    for j in range(n):\n",
    "        ngram +=  train_english_string_full[i+j]\n",
    "    create(ngram, ngram_training)\n",
    "\n",
    "#take care of the last a few characters \n",
    "n = n-1\n",
    "while n!=0 :\n",
    "    create(train_english_string_full[-n:],ngram_training)\n",
    "    n-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to find the counts in the tree structure. This function traverse the tree to the  \n",
    "#n = 10\n",
    "def find_counts (ngram, ngram_training): ## ngram_training is a tree structure for n = 10, so len(ngram) should be less than 10\n",
    "    n = len(ngram)                       ## find the counts of ngram\n",
    "    if n == 1:  ## base case\n",
    "        if ngram not in ngram_training:\n",
    "            return 0\n",
    "        else:\n",
    "            return ngram_training[ngram][0]\n",
    "    \n",
    "    else:\n",
    "        if ngram[0] not in ngram_training:\n",
    "            return 0\n",
    "        else:\n",
    "            return find_counts(ngram[1:],ngram_training[ngram[0]][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to find the subtree \n",
    "def find_subtree (ngram, ngram_training): ## ngram_training is a tree structure for n = 10, so len(ngram) should be less than 10\n",
    "    n = len(ngram)\n",
    "    if n == 1:\n",
    "        if ngram not in ngram_training:\n",
    "            temp ={}\n",
    "            return temp\n",
    "        else:\n",
    "            return ngram_training[ngram][1]\n",
    "    \n",
    "    else:\n",
    "        if ngram[0] not in ngram_training:\n",
    "            temp = {}\n",
    "            return temp\n",
    "        else:\n",
    "            return find_subtree(ngram[1:],ngram_training[ngram[0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define the recursive function for prediction with witten-bell smoothing\n",
    "## Return p(w/u)\n",
    "def prediction_witten_bell_smoothing(n,w,position_predict,word_string): ## n means n-gram, w is the cha to predict\n",
    "    ## construct u     \n",
    "    u=''\n",
    "    for i in range(n-1,0,-1):\n",
    "        u += word_string[position_predict-i]   \n",
    "    \n",
    "        \n",
    "    if len(u)==0: ## base case, return p(w)  \n",
    "        N = len(train_english_string) # number of word token-not including <s>\n",
    "        n_1_plus = len(word_count_unigram)## number of word type seen at least one time -not including <s>\n",
    "        n_word_type = len(word_count_unigram)+1 ## number of word type_include unkown word\n",
    "        lamda = N/(N+n_1_plus)\n",
    "        return lamda*ngram_training[w][0]/N + (1-lamda)/n_word_type   ## ngram_training[w][0] represents c(w), return p(w)\n",
    "        \n",
    "    #if u not in ngram_wordcount[n-1]:\n",
    "        #return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)\n",
    "    if find_counts (u, ngram_training) == 0:\n",
    "        return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)\n",
    "    else:        \n",
    "        ## calculate c(u_dot) and n1+(u_dot)        \n",
    "        sub_tree_under_u = find_subtree (u, ngram_training)\n",
    "        count_u_dot = 0\n",
    "        for i in sub_tree_under_u:\n",
    "            count_u_dot+=sub_tree_under_u[i][0]     \n",
    "        \n",
    "        count_1plus_u_dot = 0\n",
    "        for i in sub_tree_under_u:\n",
    "            count_1plus_u_dot+=1 \n",
    "            \n",
    "        if count_u_dot == 0:\n",
    "            return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)\n",
    "                \n",
    "        ## calculate lamda_u\n",
    "        lamda_u = count_u_dot/(count_u_dot+count_1plus_u_dot)\n",
    "        \n",
    "        ## calculate c(uw)\n",
    "        c_uw = find_counts(u+w, ngram_training)\n",
    "        \n",
    "        return lamda_u*c_uw/count_u_dot+(1-lamda_u)*prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in the English dev file as a big string\n",
    "with open('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/english/dev') as dev_english:\n",
    "    dev_english_string =''\n",
    "    for line in dev_english:\n",
    "        dev_english_string += line\n",
    "\n",
    "n=10\n",
    "start_word=(n-1)*\"我\"\n",
    "dev_english_string_full = start_word+dev_english_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in the English test file as a big string\n",
    "with open('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/english/test') as test_english:\n",
    "    test_english_string =''\n",
    "    for line in test_english:\n",
    "        test_english_string += line\n",
    "n=10\n",
    "start_word=(n-1)*\"我\"\n",
    "test_english_string_full = start_word+test_english_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "0.6162902914495271\n"
     ]
    }
   ],
   "source": [
    "## n=5, on dev file\n",
    "n=10 \n",
    "ngram_prediction = {}\n",
    "\n",
    "count = 0\n",
    "for position in range(n-1,len(dev_english_string_full)):\n",
    "    count += 1\n",
    "    if count == 1000:\n",
    "        print (count) \n",
    "        count = 0\n",
    "    ngram_prediction_character = {}\n",
    "    for cha in word_count_unigram:\n",
    "        ngram_prediction_character[cha]= prediction_witten_bell_smoothing(n,cha,position,dev_english_string_full)\n",
    "    ngram_prediction[position] = [key for key,val in ngram_prediction_character.items() if val == max(ngram_prediction_character.values())]\n",
    "\n",
    "count_correct_predict = 0\n",
    "for position in range(n-1,len(dev_english_string)):\n",
    "    if dev_english_string_full[position]==ngram_prediction[position][0]:\n",
    "        count_correct_predict+=1\n",
    "prediction_accuracy = count_correct_predict/(len(dev_english_string_full)-(n-1))\n",
    "\n",
    "print(prediction_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "0.6048300261498231\n"
     ]
    }
   ],
   "source": [
    "## Test n=10, on test file\n",
    "n=10 \n",
    "ngram_prediction = {}\n",
    "\n",
    "count = 0\n",
    "for position in range(n-1,len(test_english_string_full)):\n",
    "    count += 1\n",
    "    if count == 1000:\n",
    "        print (count) \n",
    "        count = 0\n",
    "    ngram_prediction_character = {}\n",
    "    for cha in word_count_unigram:\n",
    "        ngram_prediction_character[cha]= prediction_witten_bell_smoothing(n,cha,position,test_english_string_full)\n",
    "    ngram_prediction[position] = [key for key,val in ngram_prediction_character.items() if val == max(ngram_prediction_character.values())]\n",
    "\n",
    "count_correct_predict = 0\n",
    "for position in range(n-1,len(test_english_string_full)):\n",
    "    if test_english_string_full[position]==ngram_prediction[position][0]:\n",
    "        count_correct_predict+=1\n",
    "prediction_accuracy = count_correct_predict/(len(test_english_string_full)-(n-1))\n",
    "\n",
    "print(prediction_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
