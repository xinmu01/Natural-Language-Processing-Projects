{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tag_pair_counts = {}\n",
    "with open (\"C:/Users/Xin/Desktop/Github-repo/Natural_Language_Processing_Projects/Project_3_Sequence_Tagging/hw3-data/train.txt\") as train_file:\n",
    "    for line in train_file:\n",
    "        line_list = line.split()\n",
    "        for i in range(len(line_list)):\n",
    "            temp = line_list[i].split('/')\n",
    "            if temp[1] not in word_tag_pair_counts:\n",
    "                word_tag_pair_counts[temp[1]] = {}\n",
    "                word_tag_pair_counts[temp[1]][temp[0]] = 1\n",
    "            else:\n",
    "                if temp[0] not in word_tag_pair_counts[temp[1]]:\n",
    "                    word_tag_pair_counts[temp[1]][temp[0]] = 1\n",
    "                else:\n",
    "                    word_tag_pair_counts[temp[1]][temp[0]] += 1                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 389,\n",
       " '--': 17,\n",
       " '.': 40,\n",
       " '?': 7,\n",
       " '[inaudible]': 2,\n",
       " 'a': 61,\n",
       " 'able': 1,\n",
       " 'about': 16,\n",
       " 'accept': 1,\n",
       " 'actually': 4,\n",
       " 'ad': 1,\n",
       " 'adam': 1,\n",
       " 'advertisements': 1,\n",
       " 'advice': 1,\n",
       " 'after': 1,\n",
       " 'again': 1,\n",
       " 'age': 1,\n",
       " 'agent': 1,\n",
       " 'ages': 1,\n",
       " 'ago': 9,\n",
       " \"alfredo's\": 1,\n",
       " 'all': 3,\n",
       " 'almost': 1,\n",
       " 'alone': 1,\n",
       " 'along': 1,\n",
       " 'already': 1,\n",
       " 'also': 1,\n",
       " 'always': 3,\n",
       " 'am': 8,\n",
       " 'amazed': 1,\n",
       " 'american': 1,\n",
       " 'an': 2,\n",
       " 'and': 71,\n",
       " 'another': 1,\n",
       " 'answer': 1,\n",
       " 'anxious': 1,\n",
       " 'any': 1,\n",
       " 'anymore': 2,\n",
       " 'anything': 2,\n",
       " 'anyway': 2,\n",
       " 'applies': 1,\n",
       " 'are': 5,\n",
       " 'around': 3,\n",
       " 'as': 3,\n",
       " 'asked': 1,\n",
       " 'assistantship': 1,\n",
       " 'assume': 1,\n",
       " 'assumed': 1,\n",
       " 'at': 13,\n",
       " 'attacks': 1,\n",
       " 'australia': 1,\n",
       " 'away': 4,\n",
       " 'baby': 1,\n",
       " 'back': 2,\n",
       " 'basketball': 2,\n",
       " 'be': 15,\n",
       " 'because': 3,\n",
       " 'bed': 1,\n",
       " 'been': 13,\n",
       " 'before': 2,\n",
       " 'beginning': 1,\n",
       " 'believe': 2,\n",
       " 'benefits': 1,\n",
       " 'big': 2,\n",
       " 'bit': 1,\n",
       " 'blocks': 1,\n",
       " 'body': 1,\n",
       " 'book': 1,\n",
       " 'boom': 1,\n",
       " 'both': 2,\n",
       " 'bottles': 1,\n",
       " 'bottom': 1,\n",
       " 'boy': 1,\n",
       " 'brake': 1,\n",
       " 'brookhaven': 1,\n",
       " 'brought': 1,\n",
       " 'brush': 1,\n",
       " 'bush': 1,\n",
       " 'but': 49,\n",
       " 'buying': 1,\n",
       " 'by': 2,\n",
       " 'calendar': 1,\n",
       " 'call': 6,\n",
       " 'called': 5,\n",
       " 'came': 2,\n",
       " 'can': 2,\n",
       " \"can't\": 8,\n",
       " 'cannot': 1,\n",
       " 'car': 1,\n",
       " 'caravan': 1,\n",
       " 'card': 1,\n",
       " 'cards': 1,\n",
       " 'care': 2,\n",
       " 'carrier': 1,\n",
       " 'cars': 3,\n",
       " 'cases': 1,\n",
       " 'catching': 1,\n",
       " 'cent': 1,\n",
       " 'centers': 1,\n",
       " 'cents': 1,\n",
       " 'chain': 1,\n",
       " 'chance': 1,\n",
       " 'change': 1,\n",
       " 'changed': 1,\n",
       " 'charge': 1,\n",
       " 'cheap': 1,\n",
       " 'check': 1,\n",
       " 'chicago': 1,\n",
       " 'children': 3,\n",
       " 'chinese': 1,\n",
       " 'choice': 1,\n",
       " 'city': 1,\n",
       " 'closer': 1,\n",
       " 'club': 1,\n",
       " 'coast': 1,\n",
       " 'cold': 1,\n",
       " 'college': 1,\n",
       " 'come': 1,\n",
       " 'comes': 2,\n",
       " 'comment': 1,\n",
       " 'commercial': 1,\n",
       " 'concert': 1,\n",
       " 'confident': 1,\n",
       " 'containers': 1,\n",
       " 'contract': 1,\n",
       " 'could': 6,\n",
       " \"couldn't\": 3,\n",
       " 'countries': 1,\n",
       " 'country': 1,\n",
       " 'county': 1,\n",
       " 'couple': 1,\n",
       " 'course': 2,\n",
       " 'coverage': 1,\n",
       " 'credit': 1,\n",
       " 'creek': 1,\n",
       " 'crept': 1,\n",
       " 'criminal': 2,\n",
       " 'cruel': 1,\n",
       " 'curious': 1,\n",
       " 'cutting': 1,\n",
       " 'dad': 1,\n",
       " 'dallas': 1,\n",
       " 'dark': 1,\n",
       " 'day': 2,\n",
       " 'days': 1,\n",
       " 'dead': 2,\n",
       " 'detergent': 1,\n",
       " 'did': 5,\n",
       " \"didn't\": 4,\n",
       " 'dining': 1,\n",
       " 'directed': 1,\n",
       " 'distance': 1,\n",
       " 'diverted': 1,\n",
       " 'do': 23,\n",
       " 'doctor': 1,\n",
       " 'dodge': 1,\n",
       " 'does': 1,\n",
       " \"doesn't\": 2,\n",
       " 'dog': 1,\n",
       " 'doing': 3,\n",
       " 'dollars': 1,\n",
       " \"don't\": 43,\n",
       " 'down': 1,\n",
       " 'dramas': 1,\n",
       " 'drive': 1,\n",
       " 'duplicate': 1,\n",
       " 'during': 3,\n",
       " 'each': 1,\n",
       " 'earlier': 1,\n",
       " 'earth': 1,\n",
       " 'east': 2,\n",
       " 'easy': 2,\n",
       " 'eclectic': 1,\n",
       " 'eight': 2,\n",
       " 'eighties': 1,\n",
       " 'eighty': 2,\n",
       " 'eighty-five': 1,\n",
       " 'elevation': 1,\n",
       " 'eleven': 1,\n",
       " 'else': 2,\n",
       " 'end': 1,\n",
       " 'enforce': 1,\n",
       " 'entertainment': 1,\n",
       " 'europe': 2,\n",
       " 'even': 7,\n",
       " 'ever': 1,\n",
       " 'every': 3,\n",
       " 'everybody': 1,\n",
       " 'everything': 2,\n",
       " 'example': 1,\n",
       " 'excuse': 5,\n",
       " 'existed': 1,\n",
       " 'expensive': 1,\n",
       " 'expression': 1,\n",
       " 'extent': 1,\n",
       " 'face': 1,\n",
       " 'facilities': 1,\n",
       " 'fall': 1,\n",
       " 'familiar': 1,\n",
       " 'fancy': 1,\n",
       " 'father': 1,\n",
       " 'feed': 1,\n",
       " 'feel': 1,\n",
       " 'feet': 1,\n",
       " 'felt': 1,\n",
       " 'ferraro': 1,\n",
       " 'few': 3,\n",
       " 'fianc3e': 1,\n",
       " 'fifteen': 1,\n",
       " 'figuratively': 1,\n",
       " 'figure': 1,\n",
       " 'figured': 1,\n",
       " 'finished': 1,\n",
       " 'first': 2,\n",
       " 'firsthand': 1,\n",
       " 'five': 2,\n",
       " 'flag': 1,\n",
       " 'floors': 1,\n",
       " 'florida': 1,\n",
       " 'fluid': 1,\n",
       " 'food': 1,\n",
       " 'foot': 1,\n",
       " 'football': 1,\n",
       " 'for': 11,\n",
       " 'forcing': 1,\n",
       " 'forgot': 2,\n",
       " 'forgotten': 1,\n",
       " 'formal': 1,\n",
       " 'forties': 1,\n",
       " 'forty': 1,\n",
       " 'four': 4,\n",
       " 'fourteen': 1,\n",
       " 'frankly': 1,\n",
       " 'freedom': 1,\n",
       " 'friday': 1,\n",
       " 'friends': 1,\n",
       " 'from': 9,\n",
       " 'funny': 2,\n",
       " 'garden': 1,\n",
       " 'generation': 1,\n",
       " 'gentlest': 1,\n",
       " 'georgetown': 1,\n",
       " 'german': 1,\n",
       " 'get': 2,\n",
       " 'gets': 1,\n",
       " 'give': 2,\n",
       " 'given': 1,\n",
       " 'god': 3,\n",
       " 'goes': 2,\n",
       " 'going': 5,\n",
       " 'gone': 2,\n",
       " 'good': 4,\n",
       " 'gosh': 1,\n",
       " 'got': 3,\n",
       " 'grandmother': 1,\n",
       " 'group': 1,\n",
       " 'guess': 14,\n",
       " 'guy': 1,\n",
       " 'had': 4,\n",
       " 'hair': 1,\n",
       " 'happened': 1,\n",
       " 'hardest': 1,\n",
       " 'has': 6,\n",
       " 'have': 14,\n",
       " \"haven't\": 3,\n",
       " 'he': 6,\n",
       " \"he'll\": 1,\n",
       " \"he's\": 4,\n",
       " 'head': 1,\n",
       " 'hear': 1,\n",
       " 'heard': 2,\n",
       " 'her': 2,\n",
       " 'here': 6,\n",
       " 'high': 4,\n",
       " 'highland': 1,\n",
       " 'him': 1,\n",
       " 'his': 3,\n",
       " 'hispanics': 1,\n",
       " 'hold': 1,\n",
       " 'home': 3,\n",
       " 'hoover': 1,\n",
       " 'hope': 1,\n",
       " 'hospital': 1,\n",
       " 'hours': 2,\n",
       " 'house': 2,\n",
       " 'how': 12,\n",
       " 'human': 1,\n",
       " 'hundred': 4,\n",
       " 'hungry': 1,\n",
       " 'husband': 3,\n",
       " 'i': 161,\n",
       " \"i'd\": 1,\n",
       " \"i'll\": 2,\n",
       " \"i'm\": 23,\n",
       " \"i've\": 7,\n",
       " 'if': 8,\n",
       " 'imagine': 1,\n",
       " 'import': 1,\n",
       " 'in': 36,\n",
       " 'injuries': 1,\n",
       " 'insert': 1,\n",
       " 'inside': 1,\n",
       " 'insisted': 1,\n",
       " 'instruments': 1,\n",
       " 'insurance': 1,\n",
       " 'interact': 1,\n",
       " 'is': 36,\n",
       " 'it': 72,\n",
       " \"it's\": 27,\n",
       " 'job': 2,\n",
       " 'jobs': 1,\n",
       " 'july': 1,\n",
       " 'june': 1,\n",
       " 'just': 16,\n",
       " 'k': 1,\n",
       " 'keep': 1,\n",
       " 'kennel': 1,\n",
       " 'kept': 1,\n",
       " 'kid': 1,\n",
       " 'kids': 2,\n",
       " 'kind': 5,\n",
       " 'kinder': 1,\n",
       " 'knew': 1,\n",
       " 'knock': 3,\n",
       " 'know': 44,\n",
       " 'kyle': 1,\n",
       " 'l': 1,\n",
       " 'last': 6,\n",
       " 'late': 1,\n",
       " 'laundry': 1,\n",
       " 'law': 2,\n",
       " 'laws': 1,\n",
       " 'lawyer': 1,\n",
       " 'learn': 2,\n",
       " 'least': 4,\n",
       " 'let': 3,\n",
       " \"let's\": 8,\n",
       " 'light': 1,\n",
       " 'like': 8,\n",
       " 'limits': 1,\n",
       " 'line': 1,\n",
       " 'little': 8,\n",
       " 'lives': 1,\n",
       " 'long': 2,\n",
       " 'look': 2,\n",
       " 'looking': 1,\n",
       " 'lose': 1,\n",
       " 'losing': 1,\n",
       " 'lost': 1,\n",
       " 'lot': 6,\n",
       " 'luxury': 1,\n",
       " 'made': 1,\n",
       " 'majors': 1,\n",
       " 'make': 1,\n",
       " 'makes': 1,\n",
       " 'making': 2,\n",
       " 'male': 1,\n",
       " 'manners': 1,\n",
       " 'mansion': 1,\n",
       " 'many': 3,\n",
       " 'mark': 1,\n",
       " 'master': 1,\n",
       " 'may': 3,\n",
       " 'maybe': 1,\n",
       " 'me': 13,\n",
       " 'mean': 3,\n",
       " 'message': 1,\n",
       " 'met': 1,\n",
       " 'midwest': 1,\n",
       " 'might': 4,\n",
       " 'miles': 1,\n",
       " 'mine': 1,\n",
       " 'mini': 1,\n",
       " 'money': 5,\n",
       " 'months': 2,\n",
       " 'more': 5,\n",
       " 'mother': 1,\n",
       " 'moved': 2,\n",
       " 'much': 7,\n",
       " 'mulch': 1,\n",
       " 'my': 20,\n",
       " 'name': 5,\n",
       " 'names': 2,\n",
       " 'nasty': 1,\n",
       " 'near': 1,\n",
       " 'necessarily': 3,\n",
       " 'need': 3,\n",
       " 'negative': 1,\n",
       " 'never': 3,\n",
       " 'new': 1,\n",
       " 'newly': 1,\n",
       " 'news': 1,\n",
       " 'next': 2,\n",
       " 'night': 1,\n",
       " 'nine': 1,\n",
       " 'ninety': 3,\n",
       " 'no': 2,\n",
       " 'normally': 2,\n",
       " 'not': 40,\n",
       " 'now': 13,\n",
       " 'number': 1,\n",
       " 'object': 1,\n",
       " 'occasions': 1,\n",
       " 'of': 48,\n",
       " 'off': 5,\n",
       " 'offhand': 1,\n",
       " 'oh': 4,\n",
       " 'okay': 1,\n",
       " 'old': 6,\n",
       " 'on': 12,\n",
       " 'one': 12,\n",
       " 'ones': 2,\n",
       " 'only': 2,\n",
       " 'opportunity': 1,\n",
       " 'or': 15,\n",
       " 'orange': 1,\n",
       " 'order': 1,\n",
       " 'other': 5,\n",
       " 'our': 6,\n",
       " 'out': 5,\n",
       " 'over': 3,\n",
       " 'own': 2,\n",
       " 'painted': 1,\n",
       " 'palestinian': 1,\n",
       " 'paper': 2,\n",
       " 'paragraph': 1,\n",
       " 'pardon': 2,\n",
       " 'parents': 1,\n",
       " 'park': 1,\n",
       " 'part': 5,\n",
       " 'particularly': 1,\n",
       " 'passed': 1,\n",
       " 'pay': 1,\n",
       " 'pennsylvania': 1,\n",
       " 'people': 3,\n",
       " 'per': 1,\n",
       " 'percent': 1,\n",
       " 'picnic': 1,\n",
       " 'pint': 1,\n",
       " 'pittsburgh': 1,\n",
       " 'plastic': 1,\n",
       " 'please': 2,\n",
       " 'point': 1,\n",
       " 'politics': 1,\n",
       " 'polyurethane': 1,\n",
       " 'popular': 1,\n",
       " 'portfolio': 1,\n",
       " 'prefabs': 1,\n",
       " 'press': 1,\n",
       " 'pretty': 1,\n",
       " 'probably': 3,\n",
       " 'problem': 1,\n",
       " 'publication': 1,\n",
       " 'pulls': 1,\n",
       " 'puppy': 1,\n",
       " 'purchase': 1,\n",
       " 'put': 5,\n",
       " 'qualified': 1,\n",
       " 'questions': 2,\n",
       " 'quirk': 1,\n",
       " 'quite': 1,\n",
       " 'quote': 1,\n",
       " 'quotes': 1,\n",
       " 'rage': 1,\n",
       " 'ran': 2,\n",
       " 'raspy': 1,\n",
       " 'read': 1,\n",
       " 'reading': 2,\n",
       " 'real': 2,\n",
       " 'realistically': 1,\n",
       " 'really': 9,\n",
       " 'reason': 1,\n",
       " 'rec': 1,\n",
       " 'recall': 1,\n",
       " 'remember': 5,\n",
       " 'research': 1,\n",
       " 'resolve': 1,\n",
       " 'results': 1,\n",
       " 'reunion': 1,\n",
       " 'right': 10,\n",
       " 'ro-': 1,\n",
       " 'rochester': 1,\n",
       " 'room': 1,\n",
       " 'run': 1,\n",
       " 'russia': 1,\n",
       " 's': 2,\n",
       " 'sacks': 1,\n",
       " 'said': 2,\n",
       " 'salvadorians': 1,\n",
       " 'same': 2,\n",
       " 'sandinistas': 1,\n",
       " 'saw': 2,\n",
       " 'say': 12,\n",
       " 'saying': 4,\n",
       " 'scary': 1,\n",
       " 'school': 3,\n",
       " 'schools': 1,\n",
       " 'second': 2,\n",
       " 'see': 9,\n",
       " 'seem': 1,\n",
       " 'seems': 3,\n",
       " 'seen': 1,\n",
       " 'sense': 1,\n",
       " 'sent': 1,\n",
       " 'september': 1,\n",
       " 'serious': 1,\n",
       " 'seve-': 1,\n",
       " 'seven': 1,\n",
       " 'several': 1,\n",
       " 'she': 4,\n",
       " \"she's\": 4,\n",
       " \"shouldn't\": 3,\n",
       " 'show': 1,\n",
       " 'shrubs': 1,\n",
       " 'side': 1,\n",
       " 'similar': 1,\n",
       " 'since': 1,\n",
       " 'sisters': 1,\n",
       " 'sitting': 1,\n",
       " 'situation': 1,\n",
       " 'six': 2,\n",
       " 'slightly': 1,\n",
       " 'small': 1,\n",
       " 'smoke': 1,\n",
       " 'snob': 1,\n",
       " 'so': 4,\n",
       " 'sold': 1,\n",
       " 'some': 6,\n",
       " 'someone': 1,\n",
       " 'something': 4,\n",
       " 'somewhat': 1,\n",
       " 'somewhere': 1,\n",
       " 'son': 1,\n",
       " 'sons': 1,\n",
       " 'sorry': 1,\n",
       " 'sort': 1,\n",
       " 'sound': 1,\n",
       " 'span': 1,\n",
       " 'spanish': 1,\n",
       " 'speaking': 1,\n",
       " 'special': 1,\n",
       " 'speeches': 1,\n",
       " 'spend': 2,\n",
       " 'sports': 2,\n",
       " 'square': 1,\n",
       " 'star': 1,\n",
       " 'started': 1,\n",
       " 'starting': 1,\n",
       " 'starts': 1,\n",
       " 'stated': 1,\n",
       " 'steel': 1,\n",
       " 'stepfather': 1,\n",
       " 'still': 1,\n",
       " 'stopped': 1,\n",
       " 'story': 2,\n",
       " 'strange': 1,\n",
       " 'strongly': 1,\n",
       " 'stuff': 1,\n",
       " 'subject': 2,\n",
       " 'sunday': 1,\n",
       " 'support': 1,\n",
       " 'supposed': 1,\n",
       " 'supposedly': 1,\n",
       " 'sur-': 1,\n",
       " 'sure': 8,\n",
       " 'swimming': 1,\n",
       " 't': 2,\n",
       " 'take': 2,\n",
       " 'takes': 1,\n",
       " 'talk': 5,\n",
       " 'talking': 2,\n",
       " 'teach': 1,\n",
       " 'teeth': 1,\n",
       " 'telephone': 1,\n",
       " 'television': 1,\n",
       " 'tell': 4,\n",
       " 'telling': 1,\n",
       " 'ten': 1,\n",
       " 'term': 1,\n",
       " 'terminology': 2,\n",
       " 'texas': 4,\n",
       " 'than': 1,\n",
       " 'thank': 1,\n",
       " 'that': 27,\n",
       " \"that's\": 11,\n",
       " 'the': 82,\n",
       " 'their': 7,\n",
       " 'them': 10,\n",
       " 'then': 4,\n",
       " 'there': 11,\n",
       " \"there's\": 2,\n",
       " 'these': 1,\n",
       " 'they': 21,\n",
       " \"they'll\": 1,\n",
       " \"they're\": 7,\n",
       " 'thing': 5,\n",
       " 'things': 2,\n",
       " 'think': 24,\n",
       " 'thinking': 2,\n",
       " 'this': 35,\n",
       " 'those': 4,\n",
       " 'though': 2,\n",
       " 'thought': 2,\n",
       " 'thousand': 1,\n",
       " 'three': 3,\n",
       " 'through': 1,\n",
       " 'time': 10,\n",
       " 'to': 42,\n",
       " 'today': 2,\n",
       " 'tomatoes': 1,\n",
       " 'tonight': 1,\n",
       " 'too': 11,\n",
       " 'topic': 2,\n",
       " 'tractors': 1,\n",
       " 'trade': 1,\n",
       " 'travel': 1,\n",
       " 'trees': 1,\n",
       " 'true': 2,\n",
       " 'trying': 4,\n",
       " 'tuba': 1,\n",
       " 'turn': 1,\n",
       " 'turtle': 1,\n",
       " 'twenties': 1,\n",
       " 'twenty-eight': 1,\n",
       " 'two': 4,\n",
       " 'u': 2,\n",
       " 'uh': 1,\n",
       " 'undefeated': 1,\n",
       " 'understand': 1,\n",
       " 'understanding': 1,\n",
       " 'uniforms': 1,\n",
       " 'unless': 1,\n",
       " 'up': 2,\n",
       " 'upstate': 1,\n",
       " 'us': 2,\n",
       " 'use': 4,\n",
       " 'used': 2,\n",
       " 'using': 1,\n",
       " 'usually': 3,\n",
       " 'v': 1,\n",
       " 'validated': 1,\n",
       " 'very': 1,\n",
       " 'vice-president': 1,\n",
       " 'walked': 1,\n",
       " 'want': 6,\n",
       " 'war': 1,\n",
       " 'was': 32,\n",
       " \"wasn't\": 2,\n",
       " 'watched': 1,\n",
       " 'way': 6,\n",
       " 'we': 16,\n",
       " \"we'll\": 1,\n",
       " \"we're\": 7,\n",
       " \"we've\": 1,\n",
       " 'weekend': 2,\n",
       " 'weekends': 1,\n",
       " 'welfare': 1,\n",
       " 'well': 2,\n",
       " 'well-known': 1,\n",
       " 'went': 3,\n",
       " 'were': 5,\n",
       " \"weren't\": 2,\n",
       " 'west': 1,\n",
       " 'what': 37,\n",
       " \"what's\": 2,\n",
       " 'whatever': 1,\n",
       " 'when': 5,\n",
       " 'where': 1,\n",
       " 'which': 4,\n",
       " 'who': 4,\n",
       " 'whole': 1,\n",
       " 'why': 2,\n",
       " 'wife': 2,\n",
       " 'wilder': 1,\n",
       " 'will': 6,\n",
       " 'winters': 1,\n",
       " 'wisconsin': 1,\n",
       " 'wish': 1,\n",
       " 'with': 12,\n",
       " 'women': 1,\n",
       " 'wood': 3,\n",
       " 'word': 8,\n",
       " 'words': 1,\n",
       " 'work': 2,\n",
       " 'working': 1,\n",
       " 'world': 1,\n",
       " 'would': 8,\n",
       " \"wouldn't\": 1,\n",
       " 'wrong': 2,\n",
       " 'wrongly': 1,\n",
       " 'y': 1,\n",
       " 'yeah': 3,\n",
       " 'year': 8,\n",
       " \"year's\": 1,\n",
       " 'years': 13,\n",
       " 'yes': 1,\n",
       " 'you': 28,\n",
       " \"you'd\": 1,\n",
       " \"you're\": 1,\n",
       " \"you've\": 3,\n",
       " 'young': 2,\n",
       " 'your': 5,\n",
       " 'yours': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tag_pair_counts[\"A\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (\"C:/Users/Xin/Desktop/Github-repo/Natural_Language_Processing_Projects/Project_3_Sequence_Tagging/hw3-data/train.txt\") as train_file:\n",
    "    word_train = [] ## each element in the word list is a list contains the word string in one sentence \n",
    "    tag_train = [] ## each element in the tag list is a list contains the tag string in one sentence\n",
    "    for line in train_file:\n",
    "        line_list = line.split() ## by default, using whitespace as delimiter\n",
    "        word_senten = []\n",
    "        tag_senten = []\n",
    "        for i in range(len(line_list)):\n",
    "            temp = line_list[i].split('/')\n",
    "            word_senten.append(temp[0])\n",
    "            tag_senten.append(temp[1]) \n",
    "        word_train.append(word_senten)\n",
    "        tag_train.append(tag_senten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Unigram count function\n",
    "def unigram_counts(collection):\n",
    "    unigram_counts = {}\n",
    "    for i in collection:\n",
    "        for j in i:\n",
    "            if j not in unigram_counts:\n",
    "                unigram_counts[j] = 1\n",
    "            else:\n",
    "                unigram_counts[j] += 1\n",
    "    return unigram_counts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Unigram count for the tag sequence\n",
    "tag_unigram_counts = unigram_counts(tag_train)\n",
    "## Unigram count for the word sequence\n",
    "word_unigram_counts = unigram_counts(word_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for total counts\n",
    "def total_counts(collection):\n",
    "    counts = 0\n",
    "    for i in collection:\n",
    "        counts += collection[i]\n",
    "    return counts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for calculate the frequency p(t) and p(w) \n",
    "def frequency(collection,T_count):\n",
    "    for i in collection:\n",
    "        collection[i] = collection[i]/T_count\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totoal_number_tag = total_counts(tag_unigram_counts)\n",
    "totoal_number_word = total_counts(word_unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_unigram_frequency = frequency(tag_unigram_counts,totoal_number_tag)\n",
    "word_unigram_frequency = frequency(word_unigram_counts,totoal_number_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.0019051415557901528,\n",
       " 'D': 0.04110893071257953,\n",
       " 'E': 0.005422422698724184,\n",
       " 'F': 0.04907782974825095,\n",
       " 'N': 0.8385194472197194,\n",
       " 'R': 0.06396622806493578}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_unigram_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Size of vocabulary, including the unknown word\n",
    "V = len(word_unigram_counts)+1\n",
    "## Caculate P(w/t)= (c(t,w)+1)/(sum(c(t,w))+V), and P(t) \n",
    "def calucate_frequency (t, w):\n",
    "    ## Calculate sum of c(t,w)\n",
    "    sum_t_w = 0\n",
    "    for i in word_tag_pair_counts[t]:\n",
    "        sum_t_w += word_tag_pair_counts[t][i]\n",
    "    return (word_tag_pair_counts[t][i] + 1)/(sum_t_w + V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_list = [i for i in tag_unigram_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_freq_you = {}\n",
    "for i in tag_list:\n",
    "    store_freq_you[i]=calucate_frequency(i,'you')\n",
    "## Normalization\n",
    "sum_fre = 0\n",
    "for i in store_freq_you:\n",
    "    sum_fre += store_freq_you[i]\n",
    "for i in store_freq_you:\n",
    "    store_freq_you[i] = store_freq_you[i]/sum_fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F': 0.16890214839669268, 'N': 0.024560685789151115, 'A': 0.35498659263263044, 'D': 0.09693003462074834, 'E': 0.28654394428400787, 'R': 0.06807659427676956}\n"
     ]
    }
   ],
   "source": [
    "print (store_freq_you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
