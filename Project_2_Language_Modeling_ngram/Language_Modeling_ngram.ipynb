{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in the English training file as a big string\n",
    "train_english_string =''\n",
    "with open ('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/english/train') as train_english:\n",
    "    for line in train_english:\n",
    "        train_english_string += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct unigram word count for the train string\n",
    "word_count_unigram = {} ## This contains the word counts for unigram\n",
    "for cha in train_english_string:\n",
    "    if cha not in word_count_unigram:\n",
    "        word_count_unigram[cha]=1\n",
    "    else:\n",
    "        word_count_unigram[cha]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct the word string with start word <s>. Using “我” to represent <s>\n",
    "n = 10 ## This is a 10-gram model\n",
    "start_word=(n-1)*\"我\"  ## 我 represent start word <s>\n",
    "#print (start_word)\n",
    "train_english_string_full = start_word+train_english_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define function to create the tree structure to store the training parameters\n",
    "def create(ngram, dic):## ngram is a ngram string, dic is a dictionary. \n",
    "    n = len(ngram)\n",
    "    if n == 0: ##base case \n",
    "        return\n",
    "    if ngram[0] not in dic:\n",
    "        dic[ngram[0]] = []\n",
    "        dic[ngram[0]].append(1)\n",
    "        temp_dict = {}\n",
    "        dic[ngram[0]].append(temp_dict)\n",
    "    else:\n",
    "        dic[ngram[0]][0] += 1\n",
    "    create(ngram[1:],dic[ngram[0]][1])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Train the n-gram model\n",
    "n = 10  ## 10-gram model\n",
    "ngram_training = {} ## This is tree structure containing the traing results\n",
    "                    ## Key is the cha, then value is a list; 1st element in the list is the count of the key,\n",
    "                    ## 2nd element in the list is a dict in which key is the cha left following the previous key\n",
    "                    ## for n gram, there are n levels of the tree\n",
    "\n",
    "for i in range(len(train_english_string_full)-(n-1)):\n",
    "    ngram = ''\n",
    "    for j in range(n):\n",
    "        ngram +=  train_english_string_full[i+j]\n",
    "    create(ngram, ngram_training)\n",
    "\n",
    "#Deal with the last a few characters \n",
    "n = n-1\n",
    "while n!=0 :\n",
    "    create(train_english_string_full[-n:],ngram_training)\n",
    "    n-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to find the counts in the tree structure. This function traverse the tree to find the counts of ngram  \n",
    "def find_counts (ngram, ngram_training): ## ngram_training is a tree structure for n = 10, so len(ngram) should be less than 10\n",
    "    n = len(ngram)                       ## find the counts of ngram\n",
    "    if n == 1:  ## base case\n",
    "        if ngram not in ngram_training:\n",
    "            return 0\n",
    "        else:\n",
    "            return ngram_training[ngram][0]    \n",
    "    else:\n",
    "        if ngram[0] not in ngram_training:\n",
    "            return 0\n",
    "        else:\n",
    "            return find_counts(ngram[1:],ngram_training[ngram[0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to find the subtree \n",
    "def find_subtree (ngram, ngram_training): ## ngram_training is a tree structure for n = 10, so len(ngram) should be less than 10\n",
    "    n = len(ngram)\n",
    "    if n == 1:\n",
    "        if ngram not in ngram_training:\n",
    "            temp ={}\n",
    "            return temp\n",
    "        else:\n",
    "            return ngram_training[ngram][1]    \n",
    "    else:\n",
    "        if ngram[0] not in ngram_training:\n",
    "            temp = {}\n",
    "            return temp\n",
    "        else:\n",
    "            return find_subtree(ngram[1:],ngram_training[ngram[0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define the recursive function for prediction with witten-bell smoothing\n",
    "## This function returns p(w/u)\n",
    "def prediction_witten_bell_smoothing(n,w,position_predict,word_string): ## n means n-gram, w is the character to predict, \n",
    "                                                                        ## position_predict is the postion to be predicted                                                                       \n",
    "    ## construct u     \n",
    "    u=''\n",
    "    for i in range(n-1,0,-1):\n",
    "        u += word_string[position_predict-i]   \n",
    "    \n",
    "        \n",
    "    if len(u)==0: ## base case, return p(w)  \n",
    "        N = len(train_english_string) # number of word token-not including <s>\n",
    "        n_1_plus = len(word_count_unigram)## number of word type seen at least one time -not including <s>\n",
    "        n_word_type = len(word_count_unigram)+1 ## number of word type_include unkown word\n",
    "        lamda = N/(N+n_1_plus)\n",
    "        return lamda*ngram_training[w][0]/N + (1-lamda)/n_word_type   ## ngram_training[w][0] represents c(w), return p(w)\n",
    "        \n",
    "    #if u not in ngram_wordcount[n-1]:\n",
    "        #return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)\n",
    "    if find_counts (u, ngram_training) == 0:\n",
    "        return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)\n",
    "    else:        \n",
    "        ## calculate c(u_dot) and n1+(u_dot)        \n",
    "        sub_tree_under_u = find_subtree (u, ngram_training)\n",
    "        count_u_dot = 0\n",
    "        for i in sub_tree_under_u:\n",
    "            count_u_dot+=sub_tree_under_u[i][0]     \n",
    "        \n",
    "        count_1plus_u_dot = 0\n",
    "        for i in sub_tree_under_u:\n",
    "            count_1plus_u_dot+=1 \n",
    "            \n",
    "        if count_u_dot == 0:\n",
    "            return prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)\n",
    "                \n",
    "        ## calculate lamda_u\n",
    "        lamda_u = count_u_dot/(count_u_dot+count_1plus_u_dot)\n",
    "        \n",
    "        ## calculate c(uw)\n",
    "        c_uw = find_counts(u+w, ngram_training)\n",
    "        \n",
    "        return lamda_u*c_uw/count_u_dot+(1-lamda_u)*prediction_witten_bell_smoothing(n-1,w,position_predict,word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in the English dev file as a big string\n",
    "with open('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/english/dev') as dev_english:\n",
    "    dev_english_string =''\n",
    "    for line in dev_english:\n",
    "        dev_english_string += line\n",
    "\n",
    "n=10\n",
    "start_word=(n-1)*\"我\"\n",
    "dev_english_string_full = start_word+dev_english_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in the English test file as a big string\n",
    "with open('C:/Users/Xin/Desktop/NLP/Homework2/hw2-data/english/test') as test_english:\n",
    "    test_english_string =''\n",
    "    for line in test_english:\n",
    "        test_english_string += line\n",
    "n=10\n",
    "start_word=(n-1)*\"我\"\n",
    "test_english_string_full = start_word+test_english_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['i', 0.6979471503647389], 2: ['o', 0.1461454742412194], 3: ['n', 0.29419482257739776], 4: ['t', 0.36402328733414724], 5: ['i', 0.620071963982971], 6: ['c', 0.9478949322653932], 7: ['i', 0.9999014440869798], 8: ['u', 0.9999933028641393], 9: ['s', 0.9999997718863204], 10: [':', 0.7368416741212633]}\n"
     ]
    }
   ],
   "source": [
    "## Print out the most probable characters and their probabilities for the first ten position in the dev file\n",
    "\n",
    "n = 10\n",
    "ngram_prediction = {} ## This stores the most probable characters and their probabilities for the first ten positions\n",
    "\n",
    "for position in range(9,19): ## The first 10 position in dev file not including the start word\n",
    "    prob_each_character = {}\n",
    "    for cha in word_count_unigram:\n",
    "        prob_each_character[cha]= prediction_witten_bell_smoothing(n,cha,position,dev_english_string_full)\n",
    "    ngram_prediction[position-8] = [key for key,val in prob_each_character.items() if val == max(prob_each_character.values())]\n",
    "    ngram_prediction[position-8].append(prob_each_character[ngram_prediction[position-8][0]]) \n",
    "    \n",
    "print (ngram_prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy on Dev File:0.6162902914495271\n"
     ]
    }
   ],
   "source": [
    "## n=10, Prediction on dev file\n",
    "n=10 \n",
    "ngram_prediction = {}\n",
    "\n",
    "#count = 0\n",
    "for position in range(n-1,len(dev_english_string_full)):\n",
    "    #count += 1\n",
    "    #if count == 1000:\n",
    "        #print (count) ## The print out the process of the prediction\n",
    "        #count = 0\n",
    "    ngram_prediction_character = {}\n",
    "    for cha in word_count_unigram:\n",
    "        ngram_prediction_character[cha]= prediction_witten_bell_smoothing(n,cha,position,dev_english_string_full)\n",
    "    ngram_prediction[position] = [key for key,val in ngram_prediction_character.items() if val == max(ngram_prediction_character.values())]\n",
    "\n",
    "count_correct_predict = 0\n",
    "for position in range(n-1,len(dev_english_string)):\n",
    "    if dev_english_string_full[position]==ngram_prediction[position][0]:\n",
    "        count_correct_predict+=1\n",
    "prediction_accuracy = count_correct_predict/(len(dev_english_string_full)-(n-1))\n",
    "\n",
    "print(\"Prediction Accuracy on Dev File:{}\".format(prediction_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy on Test File: 0.6048300261498231\n"
     ]
    }
   ],
   "source": [
    "## Test n=10, on test file\n",
    "n=10 \n",
    "ngram_prediction = {}\n",
    "\n",
    "#count = 0\n",
    "for position in range(n-1,len(test_english_string_full)):\n",
    "    #count += 1\n",
    "    #if count == 1000:\n",
    "        #print (count) \n",
    "        #count = 0\n",
    "    ngram_prediction_character = {}\n",
    "    for cha in word_count_unigram:\n",
    "        ngram_prediction_character[cha]= prediction_witten_bell_smoothing(n,cha,position,test_english_string_full)\n",
    "    ngram_prediction[position] = [key for key,val in ngram_prediction_character.items() if val == max(ngram_prediction_character.values())]\n",
    "\n",
    "count_correct_predict = 0\n",
    "for position in range(n-1,len(test_english_string_full)):\n",
    "    if test_english_string_full[position]==ngram_prediction[position][0]:\n",
    "        count_correct_predict+=1\n",
    "prediction_accuracy = count_correct_predict/(len(test_english_string_full)-(n-1))\n",
    "\n",
    "print(\"Prediction Accuracy on Test File:{}\" .format(prediction_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculate the probability of each character in the test file\n",
    "n=10\n",
    "Probability_each_character = {}\n",
    "for position in range(n-1,len(test_english_string_full)):\n",
    "    Probability_each_character[test_english_string_full[position]]= prediction_witten_bell_smoothing(n,test_english_string_full[position],position,test_english_string_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity = 1.0467674194667902\n"
     ]
    }
   ],
   "source": [
    "## Calculate 10-gram model's perplexity on the test data set\n",
    "summation_log_prob = 0\n",
    "\n",
    "for cha in Probability_each_character:\n",
    "    summation_log_prob += np.log(Probability_each_character[cha])\n",
    "    \n",
    "perplexity = np.exp(-summation_log_prob/len(test_english_string))\n",
    "print (\"Perplexity = {}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
